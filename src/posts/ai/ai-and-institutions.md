---
title: AI & Institutions
description:
  AIs are trained on data, and the data we have was produced by our current institutions. The data reflects
  the ideas and philosophies of those institutions. As such, AIs trained on that data necessarily reflect the
  ideas and philosophy of those institutions. It is the institutions themselves that are broken; AI is only a mirror.
keywords: ai, institutions, bias, justice, data, economics
---

# {title}

I am not afraid of AIs. What I am afraid of is people deferring all personal responsibility and agency to institutions. This already happens.

AIs are trained on data, and the data we have was produced by our current institutions. The data reflects the ideas and philosophies of those institutions. As such, AIs trained on that data necessarily reflect the ideas and philosophy of those institutions. It is the institutions themselves that are broken; AI is only a mirror.

## A personal anecdote

In November 2014, I moved to Chicago, IL from Nantucket, MA. At that time, I (a transgender woman) had an ID with an "F" gender marker. Come December, I went to the downtown DMV on State Street for a new ID. The bureaucrat that assisted me refused to grant me an ID with an "F" marker, explaining that they had to give me a "Male" marker because of my birth certificate. I pleaded with them, pointing out how silly it was that my valid Massachusetts ID and my own testimony were not enough to assert my own gender. They stonewalled me, replying that this was "the policy", and couldn't be changed. In tears, I gave up, and went home with a "Male" gender marker on my temporary ID.

Years later, I learned that this was not policy, and that if I had escalated the issue beyond the first bureaucrat, things may have been resolved in my favor. However, at the time, I felt completely powerless before the State. Whether the bureaucrat was malicious or merely misinformed, they relied on the power and legitimacy of their institution to absorb blame for the ludicrous outcome. At least I got to joke about how my gender was dependent up what State I was in.

For transgender people, [stories][I Emailed My Doctor 133 Times: The Crisis In the British Healthcare System] like this are not uncommon. Institutions are generally not designed to serve the needs of marginalized groups.

## AIs embody institutions

With AI, I see similar issues. AIs are trained on what is quantifiable, but not everything can be easily measured.

- _It's not that a law enforcement officer is bigoted, the AI told them this person was suspicious._
- _It's not that the loan officer relies on data that unfairly favors certain groups, the AI told them not to lend to this person._
- _It's not that our economic system serves the interests of capital at the expense of human beings, it's that humans are too expensive compared to an AI._
- _etc._

Rather than confront the iniquities in our society and institutions, the media paints AI as the source of these issues. AI technology, like computer technology in general, changes the math of "scaling." Things that used to be expensive _(calculation, quantitative analysis, information retrieval and processing)_ are getting cheaper by the day. Even so, inequality is increasing, and [56% of Americans can't afford a $1000 emergency expense][Bankrate emergency savings report].

## The act of measurement can change the thing being measured

Consider a search engine's adversarial relationship with [search engine optimization (SEO)][SEO]. A search engine employs a method to decide which results to display for a given search. For many, the web is a source of income, and so they are incentivized to make their website as attractive to the search engine's method as possible. People can game any measure chosen by the search engine, and search results worsen as people get better at it. The same thing happens with [cobras][Cobra Effect], and it's known as a "perverse incentive."

The idea is closely related to [Campbell's Law] which states:

> "The more any quantitative¬†[social indicator](https://en.wikipedia.org/wiki/Social_indicator 'Social indicator')¬†is used for social decision-making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor."

[Goodhart's Law], as often stated, simplifies the above into:

> "When a measure becomes a target, it ceases to be a good measure"

Thus, any system in which an AI is used to inform outcomes AND where the AI is trained on those outcomes, is a feedback loop; Such usage of an AI will only act to preserve the status quo.

## The future

It's not hard to imagine things getting worse in the future, in part due to AI. But it's also very difficult for me to believe that the solution to these issues is to focus on regulating AI. Rather, we should rethink our failing institutions, and decide what kind of world we want to live in.

- _How can we achieve justice despite bigotry?_
- _How can we make data-driven decisions when the only data we have was produced by an unfair system?_
- _How can we transition to a human-centered economic system?_

ü§∑‚Äç‚ôÄÔ∏è

[I Emailed My Doctor 133 Times: The Crisis In the British Healthcare System]: https://www.youtube.com/watch?v=v1eWIshUzr8
[Bankrate emergency savings report]: https://www.bankrate.com/banking/savings/emergency-savings-report/
[SEO]: https://en.wikipedia.org/wiki/Search_engine_optimization
[Cobra Effect]: https://en.wikipedia.org/wiki/Perverse_incentive#The_original_cobra_effect
[Campbell's Law]: https://en.wikipedia.org/wiki/Campbell%27s_law
[Goodhart's Law]: https://en.wikipedia.org/wiki/Goodhart%27s_law
